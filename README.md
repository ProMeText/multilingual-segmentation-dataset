
<p align="center">
  <img src="assets/multiling_banner.png" alt="Multilingual Segmentation Dataset Banner" width="100%">
</p>

[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC--BY--NC--SA--4.0-red.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)
[![GitHub release](https://img.shields.io/github/v/release/ProMeText/multilingual-segmentation-dataset?include_prereleases&label=Release)](https://github.com/ProMeText/multilingual-segmentation-dataset/releases)
[![Last Commit](https://img.shields.io/github/last-commit/ProMeText/multilingual-segmentation-dataset)](https://github.com/ProMeText/multilingual-segmentation-dataset/commits/main)
[![Issues](https://img.shields.io/github/issues/ProMeText/multilingual-segmentation-dataset)](https://github.com/ProMeText/multilingual-segmentation-dataset/issues)

# âœ‚ï¸ Multilingual Segmentation Dataset

> **From manuscripts to models: a multilingual corpus for sentence segmentation in historical prose.**

This dataset gathers carefully segmented excerpts from a wide range of textual genres â€” including narrative, didactic, legal, theological, and scholarly prose â€” spanning seven Romance and Latin languages (13thâ€“16th c.).  
Segment boundaries reflect both historical syntax and editorial conventions, making the corpus suitable for training and evaluating sentence segmentation models, as well as for cross-linguistic and diachronic analysis in NLP and digital philology.

## ğŸ“š Documentation

- âœ‚ï¸ **Segmentation Criteria**  
  â¡ï¸ [`docs/annotation_guidelines/segmentation_criteria_en.md`](docs/annotation_guidelines/segmentation_criteria_en.md)

- ğŸ§ª **Model Architecture & Training**  
  â¡ï¸ [`docs/segmentation_model.md`](docs/segmentation_model.md)

- ğŸ”§ **Processing Pipeline (Raw â†’ Segmented)**  
  â¡ï¸ [`docs/segmentation_processing_pipeline.md`](docs/segmentation_processing_pipeline.md)

- ğŸ§¾ **Annotated Examples**  
  â¡ï¸ [`docs/segmentation_exemples.md`](docs/segmentation_exemples.md)

- ğŸŒ **Data Collection & Source Tracking**  
  â¡ï¸ [`docs/data_collection_and_source_tracking.md`](docs/data_collection_notes.md)

- ğŸ”¤ **Delimiter Configuration (per language)**  
  â¡ï¸ [`docs/annotation_guidelines/main-word-delimiters.json`](docs/annotation_guidelines/main-word-delimiters.json)

## ğŸ“– Overview

This dataset was developed to train a multilingual sentence segmentation model, used as a pre-processing step in the automatic alignment of historical texts with [**Aquilign**](https://github.com/ProMeText/aquilign), a multilingual alignment tool developed by our team.  
Once the BERT-based models are trained and selected, they are integrated into the alignment workflow to segment texts based on learned boundary recognition â€” a critical step preceding alignment itself.

The segmented excerpts serve as input for Aquilign, enabling multilingual alignment across structurally and editorially diverse texts.  
A first study applying this pipeline â€” focused on *Lancelot en prose* â€” was presented in the 2024 article [*Textual Transmission without Borders*](https://2024.computational-humanities-research.org/papers/paper104/), published in the Computational Humanities Research (CHR) conference proceedings.

As the project evolved, the segmentation corpus was gradually expanded alongside the tool. Initially limited to three Romance languages â€” *Castilian (`es`), French (`fr`), and Italian (`it`)* â€” it was later enriched with **Portuguese (`pt`)**, **Catalan (`ca`)**, **Latin (`la`)**, and **English (`en`)**, thereby increasing linguistic diversity and strengthening the robustness of cross-linguistic alignment.

The corpus provides training and evaluation material for **sentence-level segmentation** in **historical prose** from the **13th to 16th centuries**.  
Texts were selected for their **genre diversity** and their ability to reflect **editorial**, **orthographic**, and **linguistic variation** across time, geography, and scribal practices.

To support **reproducibility and multilingual evaluation**, the dataset is structured by language.  
Segmented data are stored under `data/segmented/`, with language-specific files organized as follows:
- `data/segmented/pre_split/<lang>/` â€” complete segmented lines per language  
- `data/segmented/split/monolingual/<lang>/` â€” train/dev/test JSON and TXT files  
- `data/segmented/split/multilingual/` â€” multilingual train/dev/test splits  


## ğŸ§¾ Summary

| Category           | Details                                                                 |
|--------------------|-------------------------------------------------------------------------|
| **Languages**       | Latin (`la`), French (`fr`), English (`en`), Portuguese (`pt`), Catalan (`ca`), Italian (`it`), Castilian (`es`) |
| **Period Covered**  | 13thâ€“16th centuries                                                    |
| **Text Formats**    | Plain text (TXT), XML, with some material converted from HTML or PDF   |
| **Segmentation**    | Manual sentence segmentation using language-specific criteria          |
| **License**         | [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) â€“ annotations and segmentation metadata only |

## ğŸ¯ Purpose

This dataset aims to support the training of machine learning models that can detect sentence and segment boundaries in non-standardized historical texts.

Reliable segmentation is essential for:

- downstream NLP tasks such as **parsing**, **translation**, and **alignment**,
- enhancing the **accessibility** and reusability of medieval sources,
- enabling **cross-linguistic comparison** and advancing **philological and historical-linguistic research**.

ğŸ“„ For full segmentation principles, see the detailed [Segmentation Guidelines](docs/segmentation_criteria_en.md).

â¡ï¸ For model training instructions, architecture, and evaluation, see  
[**Model Documentation**](docs/segmentation_model.md).


## ğŸ”„ Processing Pipeline

The segmentation pipeline involves the following steps, from raw historical texts to segmented training data.

<p align="center">
  <img src="assets/segmentation_pipeline2.png" alt="Processing pipeline" width="50%">
</p>

See [segmentation pipeline documentation](docs/segmentation_processing_pipeline.md) for full details on each step.



## ğŸŒ Data Collection Variability Across Languages
ğŸ“¦ For notes on text acquisition, sourcing variation, and metadata standardization, see  
â¡ï¸ [docs/data_collection_notes.md](docs/data_collection_notes.md)


## ğŸ“Š Corpus Size

Below we present the data corresponding to the most recent version of the corpus size.  
Older versions can be consulted in the [release tags](https://github.com/ProMeText/multilingual-segmentation-dataset/releases).
The current version of the corpus includes segmented excerpts in **seven historical languages**, prepared for sentence segmentation tasks.  

Each excerpt is annotated using the pound sign (`Â£`) to mark **segment boundaries**, typically corresponding to sentences or syntactic units.  
The corpus does **not include part-of-speech tagging or syntactic annotation** â€” only sentence-level segmentation.


| Language         | Texts | Segmented Tokens | Segments (`Â£`) | Train/Dev/Test? |
|------------------|-------|------------------|----------------|-----------------|
| **Latin** (`la`)       | 557   | 85,888              | 8,366          | âœ…              |
| **French** (`fr`)      | 1,526 | 160,472             | 11,774         | âœ…              |
| **English** (`en`)     | 152   | 27,072              | 2,315          | âœ…              |
| **Portuguese** (`pt`)  | 987   | 101,565             | 10,477         | âœ…              |
| **Catalan** (`ca`)     | 388   | 38,441              | 2,879          | âœ…              |
| **Italian** (`it`)     | 2,649 | 85,290              | 6,347          | âœ…              |
| **Castilian** (`es`)   | 1,436 | 111,811             | 8,091          | âœ…              |
| **Total**              | 7,695 | 610,539             | 50,249         | âœ…              |

**Legend:**

- **Texts** = total number of annotated examples (i.e. segmented lines)  
- **Segmented Tokens** = total number of tokens (excluding `Â£`)  
- **Segments (`Â£`)** = total number of `Â£` delimiters â†’ i.e. segments  
- **Train/Dev/Test?** = indicates whether `train.json`, `dev.json`, and `test.json` are all present

> â„¹ï¸ This corpus focuses on **sentence segmentation only**. It does **not include POS tagging, syntactic trees, or named entity annotations**.

## ğŸ“‚ Data Location

The most up-to-date segmented data are stored in the repository under:

```bash
data/segmented/
```
- This folder contains the **current working version** of the segmented texts.  
- For **frozen snapshots** corresponding to published versions (e.g. baseline, augmented), please refer to the [release tags](https://github.com/ProMeText/multilingual-segmentation-dataset/releases).

## ğŸ™ Credits
We gratefully acknowledge the following scholars for their contributions of source material or expertise:

- **Peter Stokes & Mark Faulkner** â€“ Guidance on available Middle English corpora  
- **SadurnÃ­ MartÃ­** â€“ Support in identifying Medieval Catalan corpora  
- **Andrea Menozzi** â€“ Insights into available Medieval Italian corpora


## ğŸš§ Project Status

This corpus is part of an **ongoing project**. While it is already being used for segmentation and alignment tasks, **further improvements, refinements, and corrections are expected**.  
We welcome feedback, error reports, and contributions to help improve the resource over time.

Please note:
- Some segmentations may be revised in future updates.
- Metadata and annotations are subject to enhancement.
- Additional languages and texts will be added as the project evolves.

## ğŸ”— Related Projects
This repository is part of a broader ecosystem of tools and corpora developed for the study of medieval multilingual textual traditions:

- [Aquilign](https://github.com/ProMeText/Aquilign)  
  A clause-level multilingual alignment engine based on contextual embeddings (LaBSE), designed specifically for premodern texts.
  
- [Parallelium â€“ an aligned scriptures dataset](https://github.com/carolisteia/parallelium-scriptures-alignment-dataset/tree/main)  
  A multilingual dataset of aligned Biblical and Qurâ€™anic texts â€” spanning medieval and modern languages â€” designed for training and evaluating multilingual alignment models, especially in historical and philological contexts.

- [Lancelot par maints langages](https://github.com/carolisteia/lancelot-par-maints-langages)  
  A parallel corpus of translations of the *Lancelot en prose* in medieval French, Castilian, and Italian, segmented and aligned using the Aquilign pipeline.

- [Multilingual Aegidius](https://github.com/ProMeText/Multilingual_Aegidius)  
  A parallel corpus of translations of Aegidius Romanusâ€™ *De regimine principum* in Latin, medieval Romance languages, and English, processed using the same segmentation and alignment workflow.


## ğŸ”® Future Directions
- Extend language coverage  
- Evaluate segmentation models  
- Broaden genre and period diversity  
- Encourage interdisciplinary use

---

## ğŸ“« Contact & Contributions
- [Open an issue or pull request](https://github.com/ProMeText/Multilingual_Aegidius/issues)

- For academic collaboration, please reach out via GitHub Discussions

  
---
## ğŸ“š How to Cite This dataset

> **Citation (draft)**  
> Please cite as:  
> Macedo, C., Ing, L., & Gille Levenson, M. (2025). *Multilingual Segmentation Dataset for Historical Prose (13thâ€“16th c.)*. GitHub repository, ongoing.  
> ğŸ“Œ Formal publication and DOI pending.  
  

## ğŸ’° Funding

This work benefited from national funding managed by the **Agence Nationale de la Recherche** under the *Investissements d'avenir* programme with the reference **ANR-21-ESRE-0005 (Biblissima+)**.

> Ce travail a bÃ©nÃ©ficiÃ© d'une aide de lâ€™Ã‰tat gÃ©rÃ©e par lâ€™**Agence Nationale de la Recherche** au titre du programme dâ€™**Investissements dâ€™avenir** portant la rÃ©fÃ©rence **ANR-21-ESRE-0005 (Biblissima+)**.


<p align="center">
  <img src="https://github.com/user-attachments/assets/915c871f-fbaa-45ea-8334-2bf3dde8252d" alt="Biblissima+ Logo" width="600"/>
</p>

## ğŸ“„ Licensing

All annotations, segmentations, and metadata are released under [**CC BY-NC-SA 4.0**](https://creativecommons.org/licenses/by-nc-sa/4.0/).  
> âš ï¸ Original textual content may be subject to source-specific licenses. Refer to the `sources` and `corpus` columns in the metadata CSV.   

[Jump to compiled data CSV â¤µï¸](https://github.com/carolisteia/mulada/blob/main/data.csv)


